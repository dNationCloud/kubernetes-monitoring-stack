#
# Copyright 2020 The dNation Kubernetes Monitoring Stack Authors. All Rights Reserved.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default values for dNation Kubernetes Monitoring Stack.
# Declare variables to be passed into helm chart dependencies.

## Override the default value of 'app' label used by k8s objects
##
nameOverride: ""

## Override the deployment namespace
##
namespaceOverride: ""

## Provide a name to substitute for the full names of resources
##
fullnameOverride: ""

ssl-exporter:
  enabled: false
  serviceMonitor:
  #  We can enable the service monitor, because we have prometheus in our monitoring stack
    enabled: true

  # # SSL Exporter example config
  # # Configure external URLs to scrape
  #   externalTargets:
  #   - example.com:443
  # # Configure  kubernetes secrets to scrape
  #   secretTargets:
  # # e.g. all secrets across all namespaces
  #   - "*/*"
  # # Certificate files on control plane nodes
  #   fileTargets:
  # # Included in default values of ssl-exporter helm chart
  #   - "/etc/kubernetes/pki/**/*.crt"
  # # Certificates within kubeconfig files
  #   kubeconfigTargets:
  # # Included in default values of ssl-exporter helm chart
  #   - /etc/kubernetes/admin.conf

## Deploy a prometheus-blackbox-exporter
## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-blackbox-exporter
##
prometheus-blackbox-exporter:
  enabled: false
  releaseLabel: true
  prometheusRule:
    enabled: true
    additionalLabels:
      prometheus_rule: '1'
    rules:
    - alert: BlackboxProbeFailed
      expr: 'probe_success == 0'
      for: 5m
      labels:
        severity: critical
      annotations:
        message: 'Blackbox probe on target: {{ $labels.target }} failed'
    - alert: BlackboxSlowProbe
      expr: 'avg_over_time(probe_duration_seconds[1m]) > 5'
      for: 5m
      labels:
        severity: warning
      annotations:
        message: 'Blackbox probe on target: {{ $labels.target }} took more than 5s to complete, probe time = {{ $value }}'
    - alert: BlackboxSslCertificateWillExpireSoon
      expr: 'round((probe_ssl_earliest_cert_expiry - time()) / 86400, 0.1) < 30'
      for: 5m
      labels:
        severity: warning
      annotations:
        message: 'SSL certificate expires in {{ $value }} days'
  serviceMonitor:
    enabled: true
#    targets:
#    - name: dnation-cloud
#      url: https://dnation.cloud/

thanos:
  enabled: true
  queryFrontend:
    enabled: false
    extraFlags:
    - --query-range.split-interval=12h
    - --query-frontend.log-queries-longer-than=10s
    - --query-frontend.compress-responses
    - |-
      --query-range.response-cache-config="config":
        "max_size": "500MB"
        "max_size_items": 0
        "validity": 0s
      "type": "in-memory"
  query:
    extraFlags:
    - --query.auto-downsampling
    dnsDiscovery:
      sidecarsService: kube-prometheus-thanos-discovery
      sidecarsNamespace: "{{ .Release.Namespace }}"
  bucketweb:
    enabled: false
  compactor:
    enabled: false
    retentionResolutionRaw: 2d
    retentionResolution5m: 10d
    retentionResolution1h: 15d
    extraFlags:
    - --compact.concurrency=3
    - --downsample.concurrency=3
  storegateway:
    enabled: false
  ruler:
    enabled: false

thanosQueryEnvoySidecar:
  enabled: false

## Configure additional grafana datasources that will be provisioned as ConfigMaps
## ref: http://docs.grafana.org/administration/provisioning/#datasources
grafanaDatasourcesAsConfigMap:
  cluster-metrics:
    - name: thanos
      isDefault: true
      type: prometheus
      access: proxy
      url: "http://{{ .Release.Name }}-thanos-query:9090"
  cluster-logs:
    - name: cluster-logs
      isDefault: false
      type: loki
      url: http://loki-gateway
      jsonData:
        httpHeaderName1: 'X-Scope-OrgID'
      secureJsonData:
        httpHeaderValue1: '1'

## Deploy a dnation-kubernetes-monitoring
## ref: https://github.com/dNationCloud/kubernetes-monitoring
##
dnation-kubernetes-monitoring:
  enabled: true

  ## Deploy a Grafana dashboards and Prometheus rules for cluster monitoring
  ##
  clusterMonitoring:
    enabled: true

  ## Label of Grafana dashboard resource used for target discovery
  ##
  grafanaDashboards:
    labelGrafana:
      grafana_dashboard: '1'

  ## Label of Prometheus rule resource used for target discovery
  ##
  prometheusRules:
    labelPrometheus:
      prometheus_rule: '1'

## Deploy a loki
## ref: https://github.com/grafana/helm-charts/tree/main/charts/loki
##
loki:
  enabled: true
  auth_enabled: false
  backend:
    replicas: 2
  write:
    # -- Number of replicas for the write
    replicas: 2
  read:
    # -- Number of replicas for the read
    replicas: 2
  test:
    enabled: false

## Deploy a loki-distributed
## with s3-compatible object storage
## ref: https://github.com/grafana/helm-charts/tree/main/charts/loki-distributed
##
loki-distributed:
  enabled: false
  loki:
    schemaConfig:
      configs:
        - from: 2020-09-07
          store: boltdb-shipper
          object_store: aws
          schema: v11
          index:
            prefix: loki_index_
            period: 24h
    storageConfig:
      boltdb_shipper:
        shared_store: aws
      aws:
        s3: "s3://access_key:secret_access_key@custom_endpoint/bucket_name"
        s3forcepathstyle: true  # set to 'false' to enable virtual-hosted-style URLs

# Deploy a promtail
## ref: https://github.com/grafana/helm-charts/tree/main/charts/promtail
##
promtail:
  enabled: true
  config:
    clients:
    - url: http://loki-gateway/loki/api/v1/push
      tenant_id: 1
  # !IMPORTANT: each cluster must have promtail external label 'cluster' with unique value.
  # This will be used as cluster discriminator when logs are aggregated on observer cluster.
  extraArgs:
    - -client.external-labels=cluster=observer-cluster

## Deploy a kube-prometheus-stack
## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack
##
kube-prometheus-stack:
  enabled: true

  ## Provide a name to substitute for the full names of resources
  ##
  fullnameOverride: kube-prometheus

  ## Default rules for monitoring the cluster
  ##
  defaultRules:
    ## Labels for default rules
    ##
    labels:
      prometheus_rule: '1'

    ## Disable some kube-prometheus-stack default rules
    ## They are replaced by rules in the kubernetes monitoring project
    ##
    rules:
      general: false
      kubernetesStorage: false

    ## Add custom rule label
    ##
    additionalRuleLabels:
      alertgroup: Cluster

  ## Deploy a Prometheus instance
  ##
  prometheus:
    prometheusSpec:
      # !IMPORTANT: each cluster must have prometheus external label 'cluster' with unique value.
      # This will be used as cluster discriminator when metrics are aggregated on observer cluster.
      externalLabels:
        cluster: observer-cluster

      ## PrometheusRules to be selected for target discovery
      ##
      ruleSelector:
        matchLabels:
          prometheus_rule: '1'

      ## Enable thanos sidecar
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosspec
      ##
      thanos:
        logLevel: info
        objectStorageConfig: {}
    thanosService:
      enabled: true

  ## Component scraping scheduler
  ## ref: https://dnationcloud.github.io/kubernetes-monitoring/helpers/FAQ/#kubernetes-monitoring-shows-down-state-for-some-control-plane-components-are-control-plane-components-working-correctly
  ##
  kubeScheduler:
    service:
      port: 10259
      targetPort: 10259
    serviceMonitor:
      https: true
      ## Skip TLS certificate validation when scraping
      # insecureSkipVerify: true
      ## Name of the server to use when validating TLS certificate
      # serverName: null

  ## Component scraping controller manager
  ## ref: https://dnationcloud.github.io/kubernetes-monitoring/helpers/FAQ/#kubernetes-monitoring-shows-down-state-for-some-control-plane-components-are-control-plane-components-working-correctly
  ##
  kubeControllerManager:
    service:
      port: 10257
      targetPort: 10257
    serviceMonitor:
      https: true
      ## Skip TLS certificate validation when scraping
      # insecureSkipVerify: true
      ## Name of the server to use when validating TLS certificate
      # serverName: null

  ## Component scraping etcd
  ##
  kubeEtcd:
    ## Etcd service
    ## ref: https://dnationcloud.github.io/kubernetes-monitoring/helpers/FAQ/#kubernetes-monitoring-shows-down-state-for-some-control-plane-components-are-control-plane-components-working-correctly
    ##
    service:
      port: 2381
      targetPort: 2381

  ## Configuration for prometheus-node-exporter subchart
  ##
  prometheus-node-exporter:
    ## Extra args extended with `rootfs`file system
    ## For default extra args see ref: https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml#L1991
    ##
    extraArgs:
    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - --collector.filesystem.fs-types-exclude=^(rootfs|autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$

  ## Deploy a Grafana instance
  ##
  grafana:
    adminUser: "admin"
    adminPassword: "pass"
    ## Deploy default dashboards
    ##
    defaultDashboardsEnabled: false

    ## Define security context of grafana container
    ##
    securityContext:
      runAsNonRoot: false
      runAsUser: 0
      runAsGroup: 0
      fsGroup: 0

    ## Define command to be executed at startup of grafana container
    ##
    command:
    - "/bin/sh"
    - "-c"
    - "/etc/secrets/dnation_brand.sh && /run.sh"

    ## Inject dNation branding
    ##
    extraConfigmapMounts:
    - name: dnation-logo
      mountPath: /usr/share/grafana/public/img/dnation_logo.svg
      subPath: dnation_logo.svg
      configMap: dnation-logo
    - name: dnation-home
      mountPath: /usr/share/grafana/public/dashboards/home.json
      subPath: home.json
      configMap: dnation-home
    extraSecretMounts:
    - name: dnation-brand
      mountPath: /etc/secrets
      secretName: dnation-brand
      defaultMode: 0755

    ## Sidecar that collects the configmaps with specified label and stores them into the respective folders
    ##
    sidecar:
      dashboards:
        label: grafana_dashboard
        folder: /var/lib/grafana/dashboards/dnation
        provider:
          name: 'dNation'
          folder: 'dNation'
          disableDelete: false
          allowUiUpdates: true
      datasources:
        label: grafana_datasource
        defaultDatasourceEnabled: false

    plugins:
    - camptocamp-prometheus-alertmanager-datasource 1.0.0
    - grafana-polystat-panel 1.2.11

  ## Deploy an Alertmanager instance
  ##
  alertmanager:
    config:
      route:
        receiver: 'null'
        routes:
        - match:
            alertname: Watchdog
          receiver: 'null'
      inhibit_rules:
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname']

# Values for Openshift clusters
openshift:
  # Set to true if you use openshift cluster
  enabled: false
  # If null, the helmchart creates a new scc
  existingSecurityContextConstraints: null
  # Service accounts that kubernetes monitoring stack uses
  serviceAccounts: []
extraServices: []
  # - name: addservice
  #   namespace: targetnamespace
  #   ports:
  #   - name: portname
  #     port: portnumber
  #     targetPort: portnumber
  #   selector:
  #     app: app
  #   type: ClusterIP

extraSecrets: []
  # - name: addsecret
  #   data:
  #     auth-extra-groups: auth_group
  #     expiration: exp
  #     token-id: idtoken

extraCertificates: []
  # - name: newCertificate
  #   secretName: name
  #   issuer: issuer
  #   commonName: common_name
  #   hosts:
  #   - www.aaa.bb

extraConfigmaps: []
  # - name: load-configmap
  #   data:
  #     load_configmap.json: |-
  #       {
  #         ...
  #       }
